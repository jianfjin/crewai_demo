from langchain_core.tools import tool
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional
import json
import os
import hashlib
from typing import Dict, Any, List, Optional, Union
from datetime import datetime, timedelta
from langchain.tools import BaseTool
from pydantic import BaseModel, Field
import requests
import pyarrow as pa
import pyarrow.ipc as ipc
import io

def make_json_serializable(obj):
    """Convert pandas/numpy objects to JSON-serializable types."""
    if isinstance(obj, (np.integer, np.int64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64)):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, pd.Series):
        return obj.to_dict()
    elif isinstance(obj, pd.DataFrame):
        return obj.to_dict('records')
    elif isinstance(obj, dict):
        return {key: make_json_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [make_json_serializable(item) for item in obj]
    else:
        return obj

# Use optimized shared cache
try:
    from ..performance.shared_data_cache import get_shared_cache
    _USE_OPTIMIZED_CACHE = True
except ImportError:
    _USE_OPTIMIZED_CACHE = False
    # Fallback to simple cache
    _GLOBAL_DATA_CACHE = {}

def get_cached_data() -> pd.DataFrame:
    """
    Get cached data using the same strategy as optimized tools
    """
    if _USE_OPTIMIZED_CACHE:
        # Use optimized shared cache
        shared_cache = get_shared_cache()
        df, cache_info = shared_cache.get_or_load_data()
        return df
    else:
        # Fallback to simple cache
        global _GLOBAL_DATA_CACHE
        backend_url = "http://127.0.0.1:8000/api/v1/data/beverage_sales"

        # Generate cache key
        cache_key = hashlib.md5(backend_url.encode()).hexdigest()
        
        # Check if data is already cached
        if cache_key in _GLOBAL_DATA_CACHE:
            return _GLOBAL_DATA_CACHE[cache_key].copy()
        
        # Load and cache data
        try:
            response = requests.get(backend_url)
            response.raise_for_status()  # Raise an exception for bad status codes
            
            # Read the Arrow IPC stream from the binary response content
            with pa.ipc.open_stream(io.BytesIO(response.content)) as reader:
                table = reader.read_all()
            
            # Convert the Arrow Table to a pandas DataFrame
            df = table.to_pandas()
            
            # Cache the data
            _GLOBAL_DATA_CACHE[cache_key] = df.copy()
            return df
            
        except requests.exceptions.RequestException as e:
                # Return sample data as fallback
                return create_sample_beverage_data()

def create_sample_beverage_data() -> pd.DataFrame:
    """Create sample beverage data for analysis when no data file is available"""
    np.random.seed(42)  # For reproducible results
    
    dates = pd.date_range('2023-01-01', '2024-12-31', freq='D')
    brands = ['Coca-Cola', 'Pepsi', 'Sprite', 'Fanta', 'Dr Pepper', 'Mountain Dew']
    categories = ['Cola', 'Lemon-Lime', 'Orange', 'Energy', 'Diet']
    regions = ['North America', 'Europe', 'Asia Pacific', 'Latin America', 'Africa']
    
    data = []
    for i, date in enumerate(dates[:365]):  # One year of data
        for brand in brands:
            # Create realistic seasonal patterns
            seasonal_factor = 1 + 0.3 * np.sin(2 * np.pi * date.dayofyear / 365)
            base_sales = np.random.normal(1000, 200) * seasonal_factor
            
            category = np.random.choice(categories)
            region = np.random.choice(regions)
            
            units_sold = max(10, int(base_sales))
            price_per_unit = np.random.uniform(1.5, 3.5)
            total_revenue = units_sold * price_per_unit
            cost_per_unit = price_per_unit * np.random.uniform(0.4, 0.7)
            total_cost = units_sold * cost_per_unit
            profit = total_revenue - total_cost
            profit_margin = (profit / total_revenue * 100) if total_revenue > 0 else 0
            
            data.append({
                'sale_date': date,
                'year': date.year,
                'month': date.month,
                'quarter': f"Q{(date.month-1)//3 + 1}",
                'region': region,
                'country': f"Country_{region.replace(' ', '_')}",
                'store_id': f"STORE_{i % 100:03d}",
                'brand': brand,
                'category': category,
                'units_sold': units_sold,
                'price_per_unit': round(price_per_unit, 2),
                'total_revenue': round(total_revenue, 2),
                'cost_per_unit': round(cost_per_unit, 2),
                'total_cost': round(total_cost, 2),
                'profit': round(profit, 2),
                'profit_margin': round(profit_margin, 2)
            })
    
    return pd.DataFrame(data)

# Input schemas for LangGraph tools
class BeverageAnalysisInput(BaseModel):
    data_path: Optional[str] = Field(default=None, description="Path to beverage data file")
    analysis_type: str = Field(default="comprehensive", description="Type of analysis to perform")

class TimeSeriesAnalysisInput(BaseModel):
    data_path: Optional[str] = Field(default=None, description="Path to data file")
    date_column: str = Field(default="date", description="Name of date column")
    value_column: str = Field(default="sales", description="Name of value column")
    periods: int = Field(default=12, description="Number of periods to forecast")

class CrossSectionalAnalysisInput(BaseModel):
    data_path: Optional[str] = Field(default=None, description="Path to data file")
    group_column: str = Field(default="brand", description="Column to group by")
    value_column: str = Field(default="sales", description="Column to analyze")

class BrandPerformanceInput(BaseModel):
    data_path: Optional[str] = Field(default=None, description="Path to data file")
    brand_column: str = Field(default="brand", description="Brand column name")
    metrics: List[str] = Field(default=["sales", "market_share"], description="Metrics to analyze")

class ProfitabilityAnalysisInput(BaseModel):
    data_path: Optional[str] = Field(default=None, description="Path to data file")
    analysis_dimension: str = Field(default="brand", description="Dimension to analyze")

class KPIAnalysisInput(BaseModel):
    data_path: Optional[str] = Field(default=None, description="Path to data file")

class SalesForecastInput(BaseModel):
    data_path: Optional[str] = Field(default=None, description="Path to data file")
    periods: int = Field(default=30, description="Number of periods to forecast")
    forecast_column: str = Field(default="sales", description="Column to forecast")

class MarketShareInput(BaseModel):
    company_revenue: Optional[float] = Field(default=None, description="Company revenue")
    total_market_revenue: Optional[float] = Field(default=None, description="Total market revenue")

@tool(args_schema=BeverageAnalysisInput)
def beverage_market_analysis() -> str:
    """Analyze beverage market structure, brand performance, and category dynamics"""
    try:
        # Load data using cached approach with fallback
        df = get_cached_data()
        
        if df.empty:
            return json.dumps({
                'error': 'No data available',
                'total_brands': 0,
                'total_categories': 0,
                'total_regions': 0,
                'market_overview': 'No market data available for analysis'
            })
        
        # Basic market analysis
        total_brands = df['brand'].nunique() if 'brand' in df.columns else 0
        total_categories = df['category'].nunique() if 'category' in df.columns else 0
        total_regions = df['region'].nunique() if 'region' in df.columns else 0
        total_revenue = df['total_revenue'].sum() if 'total_revenue' in df.columns else 0
        
        # Top performing brands
        if 'brand' in df.columns and 'total_revenue' in df.columns:
            brand_performance = df.groupby('brand')['total_revenue'].sum().sort_values(ascending=False)
            top_brands = brand_performance.head(5).to_dict()
        else:
            top_brands = {}
        
        # Category analysis
        if 'category' in df.columns and 'total_revenue' in df.columns:
            category_performance = df.groupby('category')['total_revenue'].sum().sort_values(ascending=False)
            top_categories = category_performance.head(5).to_dict()
        else:
            top_categories = {}
        
        # Regional analysis
        if 'region' in df.columns and 'total_revenue' in df.columns:
            regional_performance = df.groupby('region')['total_revenue'].sum().sort_values(ascending=False)
            top_regions = regional_performance.head(5).to_dict()
        else:
            top_regions = {}
        
        analysis = {
            'total_brands': total_brands,
            'total_categories': total_categories,
            'total_regions': total_regions,
            'total_market_value': total_revenue,
            'top_brands': top_brands,
            'top_categories': top_categories,
            'top_regions': top_regions,
            'regions_covered': list(df['region'].unique()) if 'region' in df.columns else [],
            'market_overview': f"Comprehensive beverage market analysis covering {total_brands} brands across {total_categories} categories in {total_regions} regions with total market value of ${total_revenue:,.2f}"
        }
        
        return json.dumps(analysis, indent=2)
        
    except Exception as e:
        return f"Error in beverage market analysis: {str(e)}"

@tool(args_schema=BrandPerformanceInput)
def analyze_brand_performance(brand_column: str = "brand", metrics: List[str] = ["sales", "market_share"]) -> str:
    """Analyze individual brand performance metrics and market positioning"""
    try:
        # Load data using cached approach with fallback
        df = get_cached_data()
        
        if df.empty:
            return json.dumps({
                'error': 'No data available',
                'top_brands': [],
                'performance_metrics': {},
                'brand_insights': 'No brand data available for analysis'
            })
        
        # Check required columns
        required_cols = [brand_column, 'total_revenue']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            return json.dumps({
                'error': f'Missing required columns: {missing_cols}',
                'available_columns': list(df.columns),
                'brand_insights': 'Cannot perform brand analysis due to missing data columns'
            })
        
        # Brand performance analysis
        agg_dict = {'total_revenue': ['sum', 'mean']}
        
        # Add metrics from input if they exist in the data
        for metric in metrics:
            if metric in df.columns and metric not in agg_dict:
                if metric in ['profit', 'units_sold']:
                    agg_dict[metric] = ['sum', 'mean']
                else:
                    agg_dict[metric] = 'mean'
        
        # Add optional columns if they exist
        if 'profit' in df.columns and 'profit' not in agg_dict:
            agg_dict['profit'] = ['sum', 'mean']
        if 'profit_margin' in df.columns and 'profit_margin' not in agg_dict:
            agg_dict['profit_margin'] = 'mean'
        if 'units_sold' in df.columns and 'units_sold' not in agg_dict:
            agg_dict['units_sold'] = 'sum'
        
        brand_metrics = df.groupby(brand_column).agg(agg_dict).round(2)
        
        # Flatten column names
        brand_metrics.columns = ['_'.join(col).strip() for col in brand_metrics.columns]
        
        # Top performing brands
        top_brands = brand_metrics.sort_values('total_revenue_sum', ascending=False).head(10)
        
        # Performance insights
        total_brands = len(brand_metrics)
        avg_revenue_per_brand = brand_metrics['total_revenue_sum'].mean()
        top_performer = brand_metrics['total_revenue_sum'].idxmax()
        
        # Build performance metrics
        performance_metrics = {
            'highest_revenue': brand_metrics['total_revenue_sum'].max(),
            'lowest_revenue': brand_metrics['total_revenue_sum'].min(),
            'average_revenue': avg_revenue_per_brand
        }
        
        # Add profit margin metrics if available
        if 'profit_margin_mean' in brand_metrics.columns:
            performance_metrics.update({
                'highest_profit_margin': brand_metrics['profit_margin_mean'].max(),
                'average_profit_margin': brand_metrics['profit_margin_mean'].mean()
            })
        
        analysis = {
            'total_brands_analyzed': total_brands,
            'average_revenue_per_brand': avg_revenue_per_brand,
            'top_performing_brand': top_performer,
            'top_brands': list(top_brands.index[:5]),  # Just brand names
            'performance_metrics': performance_metrics,
            'growth_analysis': {
                'total_revenue': brand_metrics['total_revenue_sum'].sum(),
                'brand_count': total_brands,
                'market_concentration': 'High' if top_brands.iloc[0]['total_revenue_sum'] > avg_revenue_per_brand * 3 else 'Medium'
            },
            'brand_insights': f"Analysis of {total_brands} brands shows {top_performer} as the top performer with average revenue per brand of ${avg_revenue_per_brand:,.2f}"
        }
        
        return json.dumps(analysis, indent=2)
        
    except Exception as e:
        return f"Error in brand performance analysis: {str(e)}"

@tool(args_schema=ProfitabilityAnalysisInput)
def profitability_analysis(analysis_dimension: str = "brand") -> str:
    """Analyze profitability metrics across different dimensions"""
    try:
        # Load data using cached approach with fallback
        df = get_cached_data()
        
        if df.empty:
            return json.dumps({
                'error': 'No data available',
                'total_revenue': 0,
                'total_cost': 0,
                'profit_margin': 0,
                'roi': 0,
                'profitability_insights': 'No data available for profitability analysis'
            })
        
        # Map analysis dimensions to actual column names
        dimension_mapping = {
            'brand': 'brand',
            'category': 'category', 
            'region': 'region',
            'brand_performance': 'brand',  # Handle incorrect dimension name
            'overall': None,  # Overall analysis
            'Cola': 'category'  # Handle specific category
        }
        
        dimension_col = dimension_mapping.get(analysis_dimension, analysis_dimension)
        
        # Check required columns
        required_cols = ['total_revenue']
        if 'total_cost' not in df.columns:
            # Calculate total_cost if not available
            if 'cost_per_unit' in df.columns and 'units_sold' in df.columns:
                df['total_cost'] = df['cost_per_unit'] * df['units_sold']
            else:
                # Estimate cost as 60% of revenue
                df['total_cost'] = df['total_revenue'] * 0.6
        
        if 'profit' not in df.columns:
            df['profit'] = df['total_revenue'] - df['total_cost']
        
        if 'profit_margin' not in df.columns:
            df['profit_margin'] = (df['profit'] / df['total_revenue'] * 100).fillna(0)
        
        # Perform analysis
        if dimension_col and dimension_col in df.columns:
            # Dimension-based analysis
            profitability = df.groupby(dimension_col).agg({
                'total_revenue': 'sum',
                'total_cost': 'sum',
                'profit': 'sum',
                'profit_margin': 'mean'
            }).round(2)
            
            # Calculate ROI
            profitability['roi'] = (profitability['profit'] / profitability['total_cost'] * 100).fillna(0)
            
            # Get top performers
            top_performers = profitability.sort_values('profit', ascending=False).head(5)
            
            analysis = {
                'analysis_dimension': analysis_dimension,
                'total_revenue': profitability['total_revenue'].sum(),
                'total_cost': profitability['total_cost'].sum(),
                'total_profit': profitability['profit'].sum(),
                'average_profit_margin': profitability['profit_margin'].mean(),
                'average_roi': profitability['roi'].mean(),
                'top_performers': top_performers.to_dict(),
                'profitability_insights': f"Profitability analysis by {analysis_dimension} shows average profit margin of {profitability['profit_margin'].mean():.2f}% and ROI of {profitability['roi'].mean():.2f}%"
            }
        else:
            # Overall analysis
            total_revenue = df['total_revenue'].sum()
            total_cost = df['total_cost'].sum()
            total_profit = df['profit'].sum()
            avg_margin = df['profit_margin'].mean()
            roi = (total_profit / total_cost * 100) if total_cost > 0 else 0
            
            analysis = {
                'analysis_dimension': 'overall',
                'total_revenue': total_revenue,
                'total_cost': total_cost,
                'total_profit': total_profit,
                'profit_margin': avg_margin,
                'roi': roi,
                'profitability_insights': f"Overall profitability analysis shows profit margin of {avg_margin:.2f}% and ROI of {roi:.2f}%"
            }
        
        return json.dumps(analysis, indent=2)
        
    except Exception as e:
        return f"Error in profitability analysis: {str(e)}"

@tool(args_schema=CrossSectionalAnalysisInput)
def cross_sectional_analysis(value_column: str = "sales") -> str:
    """Perform cross-sectional analysis across different segments"""
    try:
        # Load data using cached approach with fallback
        df = get_cached_data()
        
        # Set default parameters if not provided
        if not segment_column:
            segment_column = 'brand'
        if not value_column:
            value_column = 'total_revenue'
        
        if df.empty:
            return json.dumps({
                'error': 'No data available',
                'segment_performance': {},
                'top_performers': [],
                'performance_gaps': [],
                'comparative_insights': 'No data available for cross-sectional analysis'
            })
        
        # Map common column name variations
        column_mapping = {
            'Cola_sales': 'total_revenue',
            'sales': 'total_revenue',
            'total_sales': 'total_revenue',
            'revenue': 'total_revenue'
        }
        
        # Use mapped column name if available
        actual_value_column = column_mapping.get(value_column, value_column)
        
        # Check if columns exist
        if segment_column not in df.columns:
            available_segments = [col for col in df.columns if col in ['brand', 'category', 'region', 'country']]
            if available_segments:
                segment_column = available_segments[0]
            else:
                return json.dumps({
                    'error': f'Segment column {segment_column} not found',
                    'available_columns': list(df.columns),
                    'comparative_insights': 'Cannot perform cross-sectional analysis due to missing segment column'
                })
        
        if actual_value_column not in df.columns:
            # Try to find a suitable value column
            value_columns = [col for col in df.columns if col in ['total_revenue', 'profit', 'units_sold']]
            if value_columns:
                actual_value_column = value_columns[0]
            else:
                return json.dumps({
                    'error': f'Value column {value_column} not found',
                    'available_columns': list(df.columns),
                    'comparative_insights': 'Cannot perform cross-sectional analysis due to missing value column'
                })
        
        # Perform cross-sectional analysis
        segment_performance = df.groupby(segment_column)[actual_value_column].agg([
            'sum', 'mean', 'count', 'std'
        ]).round(2)
        
        # Calculate additional metrics
        segment_performance['coefficient_of_variation'] = (
            segment_performance['std'] / segment_performance['mean'] * 100
        ).fillna(0)
        
        # Identify top and bottom performers
        top_performers = segment_performance.sort_values('sum', ascending=False).head(5)
        bottom_performers = segment_performance.sort_values('sum', ascending=True).head(3)
        
        # Calculate performance gaps
        max_performance = segment_performance['sum'].max()
        min_performance = segment_performance['sum'].min()
        performance_gap = max_performance - min_performance
        
        # Generate insights
        total_segments = len(segment_performance)
        avg_performance = segment_performance['sum'].mean()
        
        analysis = {
            'segment_column': segment_column,
            'value_column': actual_value_column,
            'total_segments': total_segments,
            'segment_performance': segment_performance.to_dict(),
            'top_performers': list(top_performers.index),
            'bottom_performers': list(bottom_performers.index),
            'performance_gaps': {
                'max_performance': max_performance,
                'min_performance': min_performance,
                'performance_gap': performance_gap,
                'gap_percentage': (performance_gap / max_performance * 100) if max_performance > 0 else 0
            },
            'statistical_summary': {
                'mean': segment_performance['sum'].mean(),
                'median': segment_performance['sum'].median(),
                'std_deviation': segment_performance['sum'].std()
            },
            'comparative_insights': f"Cross-sectional analysis of {total_segments} {segment_column} segments shows {top_performers.index[0]} as top performer with significant performance gaps across segments"
        }
        
        return json.dumps(analysis, indent=2)
        
    except Exception as e:
        return f"Error in cross-sectional analysis: {str(e)}"

@tool(args_schema=TimeSeriesAnalysisInput)
def time_series_analysis(date_column: str = "date", value_column: str = "sales", periods: int = 12) -> str:
    """Perform time series analysis on sales data"""
    try:
        # Load data using cached approach with fallback
        df = get_cached_data()
        
        if df.empty:
            return json.dumps({
                'error': 'No data available',
                'trend_analysis': {},
                'seasonal_patterns': {},
                'time_series_insights': 'No data available for time series analysis'
            })
        
        # Set default parameters if not provided
        if not date_column:
            date_column = 'sale_date'
        if not value_column:
            value_column = 'total_revenue'
        
        # Map common column name variations
        date_mapping = {
            'date': 'sale_date',
            'sales_date': 'sale_date'
        }
        
        value_mapping = {
            'sales': 'total_revenue',
            'Cola_sales': 'total_revenue',
            'total_sales': 'total_revenue',
            'revenue': 'total_revenue'
        }
        
        # Use mapped column names if available
        actual_date_column = date_mapping.get(date_column, date_column)
        actual_value_column = value_mapping.get(value_column, value_column)
        
        # Check if columns exist
        if actual_date_column not in df.columns:
            date_columns = [col for col in df.columns if 'date' in col.lower()]
            if date_columns:
                actual_date_column = date_columns[0]
            else:
                return json.dumps({
                    'error': f'Date column {date_column} not found',
                    'available_columns': list(df.columns),
                    'time_series_insights': 'Cannot perform time series analysis due to missing date column'
                })
        
        if actual_value_column not in df.columns:
            value_columns = [col for col in df.columns if col in ['total_revenue', 'profit', 'units_sold']]
            if value_columns:
                actual_value_column = value_columns[0]
            else:
                return json.dumps({
                    'error': f'Value column {value_column} not found',
                    'available_columns': list(df.columns),
                    'time_series_insights': 'Cannot perform time series analysis due to missing value column'
                })
        
        # Convert date column to datetime
        df[actual_date_column] = pd.to_datetime(df[actual_date_column])
        
        # Sort by date
        df = df.sort_values(actual_date_column)
        
        # Aggregate by date
        daily_data = df.groupby(actual_date_column)[actual_value_column].sum().reset_index()
        
        # Calculate basic statistics
        total_periods = len(daily_data)
        avg_value = daily_data[actual_value_column].mean()
        trend = 'increasing' if daily_data[actual_value_column].iloc[-1] > daily_data[actual_value_column].iloc[0] else 'decreasing'
        
        # Monthly aggregation if we have enough data
        seasonal_pattern = {}
        if total_periods > 30:
            df['month'] = df[actual_date_column].dt.to_period('M')
            monthly_data = df.groupby('month')[actual_value_column].sum()
            # Convert Period objects to strings for JSON serialization
            seasonal_pattern = {str(k): v for k, v in monthly_data.to_dict().items()}
        else:
            seasonal_pattern = {}
        
        analysis = {
            'date_column': actual_date_column,
            'value_column': actual_value_column,
            'total_periods': total_periods,
            'date_range': {
                'start_date': daily_data[actual_date_column].min().isoformat(),
                'end_date': daily_data[actual_date_column].max().isoformat()
            },
            'trend_analysis': {
                'overall_trend': trend,
                'average_value': avg_value,
                'total_value': daily_data[actual_value_column].sum(),
                'volatility': daily_data[actual_value_column].std()
            },
            'seasonal_patterns': seasonal_pattern,
            'statistical_summary': {
                'mean': daily_data[actual_value_column].mean(),
                'median': daily_data[actual_value_column].median(),
                'std_deviation': daily_data[actual_value_column].std(),
                'min_value': daily_data[actual_value_column].min(),
                'max_value': daily_data[actual_value_column].max()
            },
            'time_series_insights': f"Time series analysis shows {trend} trend over {total_periods} periods with average value of {avg_value:.2f}"
        }
        
        return json.dumps(analysis, indent=2)
        
    except Exception as e:
        return f"Error in time series analysis: {str(e)}"

@tool(args_schema=SalesForecastInput)
def forecast_sales(periods: int = 30, forecast_column: str = "sales") -> str:
    """Forecast future sales based on historical data"""
    try:
        # Load data using cached approach with fallback
        df = get_cached_data()
        
        if df.empty:
            return json.dumps({
                'error': 'No data available',
                'forecast_periods': periods,
                'forecast_values': [],
                'forecast_insights': 'No data available for sales forecasting'
            })
        
        # Map common column name variations
        column_mapping = {
            'sales': 'total_revenue',
            'Cola_sales': 'total_revenue',
            'total_sales': 'total_revenue',
            'revenue': 'total_revenue'
        }
        
        # Use mapped column name if available
        actual_forecast_column = column_mapping.get(forecast_column, forecast_column)
        
        # Check if column exists
        if actual_forecast_column not in df.columns:
            value_columns = [col for col in df.columns if col in ['total_revenue', 'profit', 'units_sold']]
            if value_columns:
                actual_forecast_column = value_columns[0]
            else:
                return json.dumps({
                    'error': f'Forecast column {forecast_column} not found',
                    'available_columns': list(df.columns),
                    'forecast_insights': 'Cannot perform forecasting due to missing forecast column'
                })
        
        # Simple forecasting based on historical average and trend
        historical_values = df[actual_forecast_column].dropna()
        
        if len(historical_values) < 2:
            return json.dumps({
                'error': 'Insufficient historical data for forecasting',
                'forecast_insights': 'Need at least 2 data points for forecasting'
            })
        
        # Calculate basic statistics
        mean_value = historical_values.mean()
        trend = (historical_values.iloc[-1] - historical_values.iloc[0]) / len(historical_values)
        
        # Generate simple linear forecast
        forecast_values = []
        for i in range(1, periods + 1):
            forecast_value = mean_value + (trend * i)
            forecast_values.append(max(0, forecast_value))  # Ensure non-negative
        
        # Calculate confidence metrics
        historical_std = historical_values.std()
        confidence_interval = 1.96 * historical_std  # 95% confidence interval
        
        analysis = {
            'forecast_column': actual_forecast_column,
            'forecast_periods': periods,
            'historical_data_points': len(historical_values),
            'forecast_values': forecast_values,
            'forecast_summary': {
                'average_forecast': np.mean(forecast_values),
                'total_forecast': sum(forecast_values),
                'trend_direction': 'increasing' if trend > 0 else 'decreasing',
                'confidence_interval': confidence_interval
            },
            'historical_baseline': {
                'historical_mean': mean_value,
                'historical_trend': trend,
                'historical_std': historical_std
            },
            'forecast_insights': f"Forecasted {periods} periods with average value of {np.mean(forecast_values):.2f} based on historical trend of {trend:.2f} per period"
        }
        
        return json.dumps(analysis, indent=2)
        
    except Exception as e:
        return f"Error forecasting sales: {str(e)}"

@tool
def calculate_roi(investment: float, revenue: float) -> str:
    """Calculate return on investment for marketing campaigns"""
    try:
        if investment <= 0:
            return json.dumps({
                'error': 'Investment must be greater than 0',
                'roi_percentage': 0
            })
        
        roi = ((revenue - investment) / investment) * 100
        
        analysis = {
            'investment': investment,
            'revenue': revenue,
            'profit': revenue - investment,
            'roi_percentage': round(roi, 2),
            'roi_interpretation': 'Positive' if roi > 0 else 'Negative',
            'roi_insights': f"ROI of {roi:.2f}% indicates {'profitable' if roi > 0 else 'unprofitable'} investment"
        }
        
        return json.dumps(analysis, indent=2)
        
    except Exception as e:
        return f"Error calculating ROI: {str(e)}"

@tool(args_schema=KPIAnalysisInput)
def analyze_kpis() -> str:
    """Analyze key performance indicators"""
    try:
        # Load data using cached approach with fallback
        df = get_cached_data()
        
        if df.empty:
            return json.dumps({
                'error': 'No data available',
                'kpis': {},
                'kpi_insights': 'No data available for KPI analysis'
            })
        
        # Calculate basic KPIs
        kpis = {}
        
        if 'total_revenue' in df.columns:
            kpis['total_revenue'] = df['total_revenue'].sum()
            kpis['average_revenue'] = df['total_revenue'].mean()
        
        if 'profit' in df.columns:
            kpis['total_profit'] = df['profit'].sum()
            kpis['average_profit'] = df['profit'].mean()
        elif 'total_revenue' in df.columns and 'total_cost' in df.columns:
            df['profit'] = df['total_revenue'] - df['total_cost']
            kpis['total_profit'] = df['profit'].sum()
            kpis['average_profit'] = df['profit'].mean()
        
        if 'profit_margin' in df.columns:
            kpis['average_profit_margin'] = df['profit_margin'].mean()
        
        if 'units_sold' in df.columns:
            kpis['total_units_sold'] = df['units_sold'].sum()
            kpis['average_units_sold'] = df['units_sold'].mean()
        
        # Brand-specific KPIs
        if 'brand' in df.columns:
            kpis['total_brands'] = df['brand'].nunique()
            if 'total_revenue' in df.columns:
                top_brand = df.groupby('brand')['total_revenue'].sum().idxmax()
                kpis['top_performing_brand'] = top_brand
        
        # Regional KPIs
        if 'region' in df.columns:
            kpis['total_regions'] = df['region'].nunique()
            if 'total_revenue' in df.columns:
                top_region = df.groupby('region')['total_revenue'].sum().idxmax()
                kpis['top_performing_region'] = top_region
        
        # Make all KPIs JSON serializable
        serializable_kpis = make_json_serializable(kpis)
        
        analysis = {
            'kpis': serializable_kpis,
            'data_points': int(len(df)),
            'kpi_insights': f"Analysis of {len(df)} data points reveals key performance metrics across business dimensions"
        }
        
        return json.dumps(analysis, indent=2)
        
    except Exception as e:
        return f"Error analyzing KPIs: {str(e)}"

@tool
def plan_budget(total_budget: float, channels: List[str] = None, priorities: List[float] = None) -> str:
    """Plan budget allocation across different channels and campaigns"""
    try:
        if channels is None:
            channels = ['Digital Marketing', 'Traditional Media', 'Social Media', 'Content Marketing', 'Events']
        
        if priorities is None:
            # Equal distribution if no priorities specified
            priorities = [1.0] * len(channels)
        
        if len(priorities) != len(channels):
            priorities = [1.0] * len(channels)
        
        # Normalize priorities
        total_priority = sum(priorities)
        normalized_priorities = [p / total_priority for p in priorities]
        
        # Calculate budget allocation
        budget_allocation = {}
        for i, channel in enumerate(channels):
            allocation = total_budget * normalized_priorities[i]
            budget_allocation[channel] = round(allocation, 2)
        
        # Calculate percentages
        percentage_allocation = {}
        for channel, amount in budget_allocation.items():
            percentage = (amount / total_budget) * 100
            percentage_allocation[channel] = round(percentage, 1)
        
        analysis = {
            'total_budget': total_budget,
            'budget_allocation': budget_allocation,
            'percentage_allocation': percentage_allocation,
            'channels': channels,
            'allocation_strategy': 'Priority-based allocation',
            'budget_insights': f"Budget of ${total_budget:,.2f} allocated across {len(channels)} channels based on priorities"
        }
        
        return json.dumps(analysis, indent=2)
        
    except Exception as e:
        return f"Error planning budget: {str(e)}"

@tool(args_schema=MarketShareInput)
def calculate_market_share(company_revenue: float = None, total_market_revenue: float = None) -> str:
    """Calculate market share based on company and total market revenue"""
    try:
        # Set default values if not provided
        if company_revenue is None:
            company_revenue = 911325.29  # Sample Coca-Cola revenue from data
        if total_market_revenue is None:
            total_market_revenue = 5509749.08  # Sample total market revenue from data
            
        if total_market_revenue <= 0:
            return json.dumps({
                'error': 'Total market revenue must be greater than 0',
                'market_share_percentage': 0
            })
        
        market_share = (company_revenue / total_market_revenue) * 100
        
        # Determine competitive position
        if market_share >= 40:
            position = 'Market Leader'
        elif market_share >= 20:
            position = 'Strong Competitor'
        elif market_share >= 10:
            position = 'Moderate Player'
        else:
            position = 'Niche Player'
        
        analysis = {
            'company_revenue': company_revenue,
            'total_market_revenue': total_market_revenue,
            'market_share_percentage': round(market_share, 2),
            'competitive_position': position,
            'market_concentration': 'High' if market_share >= 25 else 'Medium' if market_share >= 10 else 'Low',
            'market_insights': f"Market share of {market_share:.2f}% positions company as {position.lower()}"
        }
        
        return json.dumps(analysis, indent=2)
        
    except Exception as e:
        return f"Error calculating market share: {str(e)}"

@tool
def meta_analysis_tool(data_file_path: str = "beverage_sales.csv") -> str:
    """Extract metadata and distinct values from the beverage sales dataset to understand data structure and available options"""
    try:
        import pandas as pd
        import os
        
        # Try different possible paths for the data file
        possible_paths = [
            data_file_path,
            f"data/{data_file_path}",
            f"src/data/{data_file_path}",
            f"../data/{data_file_path}",
            "beverage_sales.csv",
            "data/beverage_sales.csv"
        ]
        
        df = None
        actual_path = None
        
        for path in possible_paths:
            if os.path.exists(path):
                try:
                    df = pd.read_csv(path)
                    actual_path = path
                    break
                except Exception as e:
                    continue
        
        if df is None:
            # Return mock metadata if file not found
            return _get_mock_metadata()
        
        # Extract basic metadata
        metadata = {
            "file_path": actual_path,
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "columns": list(df.columns),
            "data_types": df.dtypes.to_dict(),
            "missing_values": df.isnull().sum().to_dict(),
            "memory_usage": df.memory_usage(deep=True).sum()
        }
        
        # Extract distinct values for key categorical columns
        categorical_columns = {
            "year": "year",
            "month": "month", 
            "quarter": "quarter",
            "region": "region",
            "country": "country",
            "brand": "brand",
            "category": "category"
        }
        
        distinct_values = {}
        
        for key, col_name in categorical_columns.items():
            # Try different possible column names
            possible_col_names = [
                col_name,
                col_name.title(),
                col_name.upper(),
                col_name.lower(),
                f"{col_name}_name",
                f"{col_name}Name"
            ]
            
            found_col = None
            for possible_name in possible_col_names:
                if possible_name in df.columns:
                    found_col = possible_name
                    break
            
            if found_col:
                try:
                    unique_vals = df[found_col].dropna().unique().tolist()
                    # Limit to reasonable number and sort
                    if len(unique_vals) > 50:
                        unique_vals = sorted(unique_vals)[:50]
                    else:
                        unique_vals = sorted(unique_vals)
                    distinct_values[key] = unique_vals
                except Exception as e:
                    distinct_values[key] = []
            else:
                distinct_values[key] = []
        
        # Calculate summary statistics for numerical columns
        numerical_stats = {}
        numerical_columns = df.select_dtypes(include=['number']).columns
        
        for col in numerical_columns:
            try:
                stats = {
                    "min": float(df[col].min()),
                    "max": float(df[col].max()),
                    "mean": float(df[col].mean()),
                    "median": float(df[col].median()),
                    "std": float(df[col].std())
                }
                numerical_stats[col] = stats
            except Exception:
                continue
        
        # Compile final metadata
        final_metadata = {
            "dataset_info": metadata,
            "distinct_values": distinct_values,
            "numerical_stats": numerical_stats,
            "data_quality": {
                "completeness": (1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100,
                "total_missing": df.isnull().sum().sum(),
                "duplicate_rows": df.duplicated().sum()
            },
            "insights": _generate_insights(distinct_values, numerical_stats, metadata)
        }
        
        return json.dumps(final_metadata, indent=2, default=str)
        
    except Exception as e:
        return _get_mock_metadata()

def _get_mock_metadata() -> str:
    """Return mock metadata when actual data is not available"""
    mock_metadata = {
        "dataset_info": {
            "file_path": "beverage_sales.csv",
            "total_rows": 10000,
            "total_columns": 12,
            "columns": ["sale_date", "year", "month", "quarter", "region", "country", "brand", "category", "product", "units_sold", "revenue", "cost"],
            "data_types": {
                "sale_date": "datetime64[ns]",
                "year": "int64",
                "month": "int64", 
                "quarter": "int64",
                "region": "object",
                "country": "object",
                "brand": "object",
                "category": "object",
                "product": "object",
                "units_sold": "int64",
                "revenue": "float64",
                "cost": "float64"
            }
        },
        "distinct_values": {
            "year": [2020, 2021, 2022, 2023, 2024],
            "month": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],
            "quarter": [1, 2, 3, 4],
            "region": ["North America", "Europe", "Asia Pacific", "Latin America", "Middle East", "Africa"],
            "country": ["USA", "Canada", "UK", "Germany", "France", "Japan", "China", "Australia", "Brazil", "Mexico"],
            "brand": ["Coca-Cola", "Pepsi", "Red Bull", "Monster Energy", "Gatorade", "Powerade", "Tropicana", "Simply Orange", "Minute Maid", "Sprite", "Fanta", "7UP"],
            "category": ["Cola", "Juice", "Energy", "Sports", "Citrus", "Lemon-Lime", "Orange", "Water", "Enhanced Water"]
        },
        "numerical_stats": {
            "units_sold": {"min": 100, "max": 10000, "mean": 2500, "median": 2000, "std": 1500},
            "revenue": {"min": 500, "max": 50000, "mean": 12500, "median": 10000, "std": 8000},
            "cost": {"min": 200, "max": 20000, "mean": 5000, "median": 4000, "std": 3200}
        },
        "data_quality": {
            "completeness": 98.5,
            "total_missing": 150,
            "duplicate_rows": 5
        },
        "insights": [
            "Dataset contains 5 years of beverage sales data (2020-2024)",
            "Data covers 6 major regions with 10 countries",
            "12 major beverage brands across 9 product categories",
            "High data quality with 98.5% completeness",
            "Revenue ranges from $500 to $50,000 per transaction"
        ]
    }
    
    return json.dumps(mock_metadata, indent=2, default=str)

def _generate_insights(distinct_values: dict, numerical_stats: dict, metadata: dict) -> list:
    """Generate insights from the metadata"""
    insights = []
    
    # Time range insights
    if "year" in distinct_values and distinct_values["year"]:
        years = distinct_values["year"]
        if len(years) > 1:
            insights.append(f"Dataset spans {len(years)} years from {min(years)} to {max(years)}")
    
    # Geographic coverage
    if "region" in distinct_values and distinct_values["region"]:
        regions = distinct_values["region"]
        insights.append(f"Data covers {len(regions)} regions: {', '.join(regions[:3])}{'...' if len(regions) > 3 else ''}")
    
    # Brand coverage
    if "brand" in distinct_values and distinct_values["brand"]:
        brands = distinct_values["brand"]
        insights.append(f"Analysis includes {len(brands)} brands: {', '.join(brands[:3])}{'...' if len(brands) > 3 else ''}")
    
    # Category coverage
    if "category" in distinct_values and distinct_values["category"]:
        categories = distinct_values["category"]
        insights.append(f"Product categories: {', '.join(categories)}")
    
    # Data quality
    total_rows = metadata.get("total_rows", 0)
    if total_rows > 0:
        insights.append(f"Dataset contains {total_rows:,} records")
    
    return insights


class CustomerChurnAnalysisInput(BaseModel):
    """Input schema for customer churn analysis tool."""
    time_period: str = Field(default="quarter", description="Time period for analysis: 'quarter' or 'month'")


@tool(args_schema=CustomerChurnAnalysisInput)
def customer_churn_analysis(time_period: str = "quarter") -> str:
    """
    Analyze customer churn rates and patterns based on sales data trends.
    
    Args:
        time_period: Time period for analysis ('quarter' or 'month')
    
    Returns:
        JSON string with churn analysis results including brand-specific churn rates
    """
    try:
        # Load data using cached approach with fallback
        df = get_cached_data()
        
        if df.empty:
            return json.dumps({
                'error': 'No data available',
                'churn_analysis': {},
                'brand_churn_rates': {},
                'churn_insights': 'No data available for customer churn analysis'
            })
        
        # Since we don't have direct churn data, we'll analyze sales decline patterns
        # as a proxy for customer churn (declining sales often indicate customer loss)
        
        # Ensure we have required columns
        if 'sale_date' not in df.columns:
            return json.dumps({
                'error': 'Date column required for churn analysis',
                'churn_insights': 'Cannot analyze churn without temporal data'
            })
        
        # Convert date column
        df['sale_date'] = pd.to_datetime(df['sale_date'])
        df['year'] = df['sale_date'].dt.year
        df['quarter'] = df['sale_date'].dt.quarter
        df['month'] = df['sale_date'].dt.month
        
        # Analyze by time period
        if time_period == "quarter":
            df['period'] = df['year'].astype(str) + '-Q' + df['quarter'].astype(str)
            period_col = 'period'
        else:
            df['period'] = df['sale_date'].dt.to_period('M').astype(str)
            period_col = 'period'
        
        # Calculate churn indicators by brand
        brand_churn_analysis = {}
        
        if 'brand' in df.columns and 'total_revenue' in df.columns:
            for brand in df['brand'].unique():
                brand_data = df[df['brand'] == brand]
                
                # Calculate period-over-period revenue changes
                period_revenue = brand_data.groupby(period_col)['total_revenue'].sum().sort_index()
                
                if len(period_revenue) >= 2:
                    # Calculate period-over-period change
                    revenue_changes = period_revenue.pct_change().dropna()
                    
                    # Churn indicators:
                    # 1. Consecutive declining periods
                    declining_periods = (revenue_changes < -0.05).sum()  # More than 5% decline
                    total_periods = len(revenue_changes)
                    
                    # 2. Overall trend
                    if len(period_revenue) >= 3:
                        recent_trend = period_revenue.iloc[-3:].pct_change().mean()
                    else:
                        recent_trend = revenue_changes.mean()
                    
                    # 3. Volatility (high volatility can indicate customer instability)
                    volatility = revenue_changes.std()
                    
                    # 4. Calculate estimated churn rate based on revenue decline
                    # This is a proxy - actual churn would need customer-level data
                    severe_declines = (revenue_changes < -0.15).sum()  # 15%+ decline
                    estimated_churn_rate = (severe_declines / total_periods) * 100 if total_periods > 0 else 0
                    
                    # 5. Customer retention proxy (stable/growing revenue indicates retention)
                    stable_periods = (revenue_changes.abs() < 0.05).sum()  # Within 5% change
                    retention_proxy = (stable_periods / total_periods) * 100 if total_periods > 0 else 0
                    
                    brand_churn_analysis[brand] = {
                        'estimated_churn_rate': round(estimated_churn_rate, 2),
                        'retention_proxy': round(retention_proxy, 2),
                        'declining_periods': int(declining_periods),
                        'total_periods_analyzed': int(total_periods),
                        'recent_trend': round(recent_trend * 100, 2),  # Convert to percentage
                        'revenue_volatility': round(volatility, 3),
                        'risk_level': _assess_churn_risk(estimated_churn_rate, recent_trend, volatility),
                        'latest_revenue': float(period_revenue.iloc[-1]),
                        'revenue_change_last_period': round(revenue_changes.iloc[-1] * 100, 2) if len(revenue_changes) > 0 else 0
                    }
        
        # Sort brands by estimated churn rate (highest first)
        sorted_brands = sorted(brand_churn_analysis.items(), 
                             key=lambda x: x[1]['estimated_churn_rate'], 
                             reverse=True)
        
        # Generate insights
        if sorted_brands:
            highest_churn_brand = sorted_brands[0][0]
            highest_churn_rate = sorted_brands[0][1]['estimated_churn_rate']
            
            # Identify brands with concerning trends
            high_risk_brands = [brand for brand, data in sorted_brands 
                              if data['risk_level'] in ['High', 'Critical']]
            
            churn_insights = f"""Customer Churn Analysis (Based on Revenue Trend Patterns):

HIGHEST CHURN INDICATORS:
• {highest_churn_brand}: {highest_churn_rate}% estimated churn rate
• {len(high_risk_brands)} brands showing high churn risk

KEY FINDINGS:
• Analysis based on {len(brand_churn_analysis)} brands across {time_period} periods
• Revenue decline patterns used as churn proxy (actual churn requires customer-level data)
• Brands with consistent revenue declines indicate potential customer loss

HIGH-RISK BRANDS: {', '.join(high_risk_brands[:5]) if high_risk_brands else 'None identified'}

NOTE: This analysis uses revenue trends as a proxy for customer churn. 
For accurate churn analysis, customer-level retention data would be needed."""
        else:
            churn_insights = "Insufficient data for meaningful churn analysis"
        
        analysis = {
            'analysis_method': 'Revenue-trend-based churn proxy',
            'time_period': time_period,
            'total_brands_analyzed': len(brand_churn_analysis),
            'brand_churn_rates': dict(sorted_brands),
            'summary_statistics': {
                'average_estimated_churn': round(sum(data['estimated_churn_rate'] for _, data in sorted_brands) / len(sorted_brands), 2) if sorted_brands else 0,
                'highest_churn_rate': highest_churn_rate if sorted_brands else 0,
                'brands_at_risk': len(high_risk_brands),
                'total_brands': len(sorted_brands)
            },
            'churn_insights': churn_insights,
            'methodology_note': 'This analysis uses revenue decline patterns as a proxy for customer churn since direct customer retention data is not available in the dataset.'
        }
        
        return json.dumps(analysis, indent=2)
        
    except Exception as e:
        return f"Error in customer churn analysis: {str(e)}"


def _assess_churn_risk(churn_rate: float, trend: float, volatility: float) -> str:
    """Assess churn risk level based on multiple factors"""
    if churn_rate >= 30 or trend <= -20:
        return "Critical"
    elif churn_rate >= 20 or trend <= -15:
        return "High"
    elif churn_rate >= 10 or trend <= -10:
        return "Medium"
    elif churn_rate >= 5 or trend <= -5:
        return "Low"
    else:
        return "Minimal"
